{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topholder.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOCEQ0sneFtZy3uD5K4H9rA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s3woo/abcd1234/blob/main/topholder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8GTNelX2NxG"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "    \n",
        "RESULTS = \"results.csv\"\n",
        "URL = \"https://etherscan.io/token/generic-tokenholders2?a=0x6425c6be902d692ae2db752b3c268afadb099d3b&s=0&p=\"\n",
        "\n",
        "def getData(sess, page):\n",
        "    url = URL + page\n",
        "    print(\"Retrieving page\", page)\n",
        "    return BeautifulSoup(sess.get(url).text, 'html.parser')\n",
        "\n",
        "def getPage(sess, page):\n",
        "    table = getData(sess, str(int(page))).find('table')\n",
        "    return [[X.text.strip() for X in row.find_all('td')] for row in table.find_all('tr')]\n",
        "\n",
        "\n",
        "resp = requests.get(URL)\n",
        "sess = requests.Session()\n",
        "\n",
        "with open(RESULTS, 'wb') as f:\n",
        "     wr = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
        "     wr.writerow(map(str, \"Rank Address Quantity Percentage\".split()))\n",
        "     page = 0\n",
        "     while True:\n",
        "         page += 1\n",
        "         data = getPage(sess, page)\n",
        "\n",
        "         # Even pages that don't contain the data we're\n",
        "         # after still contain a table.\n",
        "          if len(data) < 5:\n",
        "                break\n",
        "            else:\n",
        "                for row in data:\n",
        "                wr.writerow(row)\n",
        "                time.sleep(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jlIf14snZpk"
      },
      "source": [
        "from requests import Request, Session\n",
        "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
        "import pandas as pd\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re # 추가\n",
        "from urllib.request import urlopen\n",
        "import requests\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX2PBUQe2u2a"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3_CsRRWnaAL"
      },
      "source": [
        "csv_test = pd.read_csv('result_0.csv')\n",
        "contract = csv_test['platform.token_address']\n",
        "print (contract)\n",
        "\n",
        "datas = csv_test.to_dict('records')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUtM60d7jCiF"
      },
      "source": [
        "import time\n",
        "import random\n",
        "for data in datas:\n",
        "    line = data['platform.token_address']\n",
        "    print(line)\n",
        "    url = 'https://etherscan.io/token/tokenholderchart/'+line\n",
        "    req = Request(url, headers={'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'})   # I got this line from another post since \"uClient = uReq(URL)\" and \"page_html = uClient.read()\" would not work (I beleive that etherscan is attemption to block webscraping or something?)\n",
        "    response = urlopen(req, timeout=20).read()\n",
        "    response_close = urlopen(req, timeout=20).close()\n",
        "    page_soup = soup(response, \"html.parser\")\n",
        "    try:\n",
        "        Transfers_info_table_1 = page_soup.find(\"table\", {\"class\": \"table table-hover\"})\n",
        "        df=pd.read_html(str(Transfers_info_table_1))[0]\n",
        "    except:\n",
        "        top10 = 'error'\n",
        "        data['top10_holders'] = top10\n",
        "        print(line,top10)\n",
        "    else:\n",
        "        try:\n",
        "            df['Percentage'] = df['Percentage'].str.replace('%','')\n",
        "            df=df.astype({'Percentage':'float'})\n",
        "            top10 = df['Percentage'][0] + df['Percentage'][1] + df['Percentage'][2] + df['Percentage'][3] + df['Percentage'][4] + df['Percentage'][5] + df['Percentage'][6] + df['Percentage'][7] + df['Percentage'][8] + df['Percentage'][9]\n",
        "            data['top10_holders'] = top10\n",
        "            print(line,top10)\n",
        "        except:\n",
        "            top10 = 'holder error'\n",
        "            data['top10_holders'] = top10\n",
        "            print(line,top10)           \n",
        "\n",
        "\n",
        "df.to_csv(\"TransferTable.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcpbHyiIc4QL"
      },
      "source": [
        "import requests\n",
        "import time\n",
        "import random\n",
        "for data in datas:\n",
        "    line = data['platform.token_address']\n",
        "    print(line)\n",
        "    url = 'https://etherscan.io/token/tokenholderchart/'+line\n",
        "    req = Request(url, headers={'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'})   # I got this line from another post since \"uClient = uReq(URL)\" and \"page_html = uClient.read()\" would not work (I beleive that etherscan is attemption to block webscraping or something?)\n",
        "    #response = urlopen(req, timeout=20).read()\n",
        "    #response_close = urlopen(req, timeout=20).close()\n",
        "    resp = requests.get(url)\n",
        "    sess = requests.Session()\n",
        "    page_soup = BeautifulSoup(sess.get(url).text, 'html.parser')\n",
        "\n",
        "    print (page_soup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quD-_2EtcX2_"
      },
      "source": [
        "\n",
        "    try:\n",
        "        Transfers_info_table_1 = page_soup.find(\"table\", {\"class\": \"table table-hover\"})\n",
        "        df=pd.read_html(str(Transfers_info_table_1))[0]\n",
        "    except:\n",
        "        top10 = 'error'\n",
        "        data['top10_holders'] = top10\n",
        "        print(line,top10)\n",
        "    else:\n",
        "        try:\n",
        "            df['Percentage'] = df['Percentage'].str.replace('%','')\n",
        "            df=df.astype({'Percentage':'float'})\n",
        "            top10 = df['Percentage'][0] + df['Percentage'][1] + df['Percentage'][2] + df['Percentage'][3] + df['Percentage'][4] + df['Percentage'][5] + df['Percentage'][6] + df['Percentage'][7] + df['Percentage'][8] + df['Percentage'][9]\n",
        "            data['top10_holders'] = top10\n",
        "            print(line,top10)\n",
        "        except:\n",
        "            top10 = 'holder error'\n",
        "            data['top10_holders'] = top10\n",
        "            print(line,top10)           \n",
        "\n",
        "\n",
        "df.to_csv(\"TransferTable.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLORLDcUXQqR"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "filePath = '/content/'  # 폴더 주소 입력\n",
        "fileAll = os.listdir(filePath)\n",
        "\n",
        "for file in fileAll:\n",
        "    rowsize = sum(1 for row in (open(filePath + file, encoding='UTF-8')))\n",
        "    newsize = 100   # 쪼개고 싶은 행수 수준으로 입력. 이정도 행수는 200mb 이하임.\n",
        "    times = 0\n",
        "    for i in range(1, rowsize, newsize):\n",
        "        times += 1   # 폴더 내 파일을 하나씩 점검하면서, 입력한 newsize보다 넘는 행을 쪼개줌\n",
        "        df = pd.read_csv(filePath + file, header=None, nrows = newsize, skiprows=i)\n",
        "        csv_output = file[:-4] + '_' + str(times) + '.csv'   # 쪼갠 수만큼 _1, _2... _n으로 꼬리를 달아서 파일명이 저장됨\n",
        "        df.to_csv(filePath + csv_output, index=False, header=False, mode='a', chunksize=rowsize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00fA4PTQ0Cab"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM29tk5fjuMd"
      },
      "source": [
        "df=df.astype({'Percentage':'float'})\n",
        "print(df.dtypes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-P9nf4ZkmXr"
      },
      "source": [
        "df['Percentage'] = df['Percentage'].str.replace('%','')\n",
        "df['Percentage'].sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}